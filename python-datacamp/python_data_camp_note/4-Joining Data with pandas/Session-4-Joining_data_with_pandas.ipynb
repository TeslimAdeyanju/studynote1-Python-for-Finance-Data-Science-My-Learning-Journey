{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Libraries import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = r'/Users/teslim/OneDrive/generaldata/15_data_Ch_4_1.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fund_id</th>\n",
       "      <th>Beta</th>\n",
       "      <th>RiskFreeRate</th>\n",
       "      <th>MarkertReturnRate</th>\n",
       "      <th>StandardDeviation</th>\n",
       "      <th>Returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fund_ 1</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>-0.057763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fund_ 2</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.023262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fund_ 3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>-0.027454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fund_ 4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.040554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fund_ 5</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>-0.034595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fund_id   Beta   RiskFreeRate   MarkertReturnRate   StandardDeviation   \\\n",
       "0  Fund_ 1    0.77          0.035             0.0543               0.0521   \n",
       "1  Fund_ 2    0.73          0.035             0.0543               0.0346   \n",
       "2  Fund_ 3    1.00          0.035             0.0543               0.0445   \n",
       "3  Fund_ 4    0.94          0.035             0.0543               0.0485   \n",
       "4  Fund_ 5    1.02          0.035             0.0543               0.0579   \n",
       "\n",
       "    Returns  \n",
       "0 -0.057763  \n",
       "1  0.023262  \n",
       "2 -0.027454  \n",
       "3  0.040554  \n",
       "4 -0.034595  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_page_1 = pd.read_excel(file_path, sheet_name=1)\n",
    "data_file_page_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fund_id</th>\n",
       "      <th>Returns</th>\n",
       "      <th>NAV</th>\n",
       "      <th>fund_age</th>\n",
       "      <th>expenses_ratio</th>\n",
       "      <th>manager_tenure</th>\n",
       "      <th>fund_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fund_ 1</td>\n",
       "      <td>-0.057763</td>\n",
       "      <td>537.77</td>\n",
       "      <td>36.38</td>\n",
       "      <td>1.41</td>\n",
       "      <td>8.33</td>\n",
       "      <td>523.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fund_ 2</td>\n",
       "      <td>0.023262</td>\n",
       "      <td>199.24</td>\n",
       "      <td>25.78</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>780.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fund_ 3</td>\n",
       "      <td>-0.027454</td>\n",
       "      <td>164.00</td>\n",
       "      <td>39.84</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>160.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fund_ 4</td>\n",
       "      <td>0.040554</td>\n",
       "      <td>117.62</td>\n",
       "      <td>18.06</td>\n",
       "      <td>3.51</td>\n",
       "      <td>18.67</td>\n",
       "      <td>117.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fund_ 5</td>\n",
       "      <td>-0.034595</td>\n",
       "      <td>53.98</td>\n",
       "      <td>56.31</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.42</td>\n",
       "      <td>5315.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fund_id   Returns     NAV  fund_age  expenses_ratio  manager_tenure  \\\n",
       "0  Fund_ 1 -0.057763  537.77     36.38            1.41            8.33   \n",
       "1  Fund_ 2  0.023262  199.24     25.78            1.05            0.92   \n",
       "2  Fund_ 3 -0.027454  164.00     39.84            1.99            2.00   \n",
       "3  Fund_ 4  0.040554  117.62     18.06            3.51           18.67   \n",
       "4  Fund_ 5 -0.034595   53.98     56.31            6.85            7.42   \n",
       "\n",
       "   fund_size  \n",
       "0     523.84  \n",
       "1     780.56  \n",
       "2     160.86  \n",
       "3     117.62  \n",
       "4    5315.00  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_page_2 = pd.read_excel(file_path, sheet_name=2)\n",
    "data_file_page_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fund_id</th>\n",
       "      <th>beta</th>\n",
       "      <th>riskfreerate</th>\n",
       "      <th>markertreturnrate</th>\n",
       "      <th>standarddeviation</th>\n",
       "      <th>returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fund_ 1</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>-0.057763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fund_ 2</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.023262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fund_ 3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>-0.027454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fund_ 4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.040554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fund_ 5</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>-0.034595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fund_ 6</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>0.029320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fund_ 7</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.023787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fund_ 8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.019570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fund_ 9</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.010891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fund_ 10</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.027380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fund_ 11</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0562</td>\n",
       "      <td>0.001506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fund_ 12</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.039334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fund_ 13</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0511</td>\n",
       "      <td>-0.018080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fund_ 14</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.042553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Fund_ 15</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.053575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fund_ 16</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.011799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Fund_ 17</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>-0.033941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fund_ 18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.026713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Fund_ 19</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.017146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fund_ 20</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0373</td>\n",
       "      <td>0.018682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fund_ 21</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.019504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Fund_ 22</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.020009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Fund_ 23</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.055120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Fund_ 24</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>0.040455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fund_ 25</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.057316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Fund_ 26</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.025105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Fund_ 27</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.027650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fund_ 28</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.014087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Fund_ 29</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.027902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fund_ 30</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.098552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Fund_ 31</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.026166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Fund_ 32</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0531</td>\n",
       "      <td>0.044948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Fund_ 33</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.014087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Fund_ 34</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.041309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Fund_ 35</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0448</td>\n",
       "      <td>0.051406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Fund_ 36</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.023485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Fund_ 37</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.025625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Fund_ 38</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.037007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Fund_ 39</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.017190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Fund_ 40</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Fund_ 41</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.037782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Fund_ 42</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>0.044438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Fund_ 43</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.051556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Fund_ 44</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.039726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Fund_ 45</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.002745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Fund_ 46</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.044673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Fund_ 47</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.050250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Fund_ 48</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.028529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Fund_ 49</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.044205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Fund_ 50</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.040828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fund_id  beta  riskfreerate  markertreturnrate  standarddeviation  \\\n",
       "0    Fund_ 1  0.77         0.035             0.0543             0.0521   \n",
       "1    Fund_ 2  0.73         0.035             0.0543             0.0346   \n",
       "2    Fund_ 3  1.00         0.035             0.0543             0.0445   \n",
       "3    Fund_ 4  0.94         0.035             0.0543             0.0485   \n",
       "4    Fund_ 5  1.02         0.035             0.0543             0.0579   \n",
       "5    Fund_ 6  0.97         0.035             0.0543             0.0438   \n",
       "6    Fund_ 7  0.97         0.035             0.0543             0.0460   \n",
       "7    Fund_ 8  0.88         0.035             0.0543             0.0588   \n",
       "8    Fund_ 9  1.13         0.035             0.0543             0.0534   \n",
       "9   Fund_ 10  0.80         0.035             0.0543             0.0364   \n",
       "10  Fund_ 11  0.97         0.035             0.0543             0.0562   \n",
       "11  Fund_ 12  0.97         0.035             0.0543             0.0519   \n",
       "12  Fund_ 13  1.04         0.035             0.0543             0.0511   \n",
       "13  Fund_ 14  1.09         0.035             0.0543             0.0485   \n",
       "14  Fund_ 15  0.72         0.035             0.0543             0.0372   \n",
       "15  Fund_ 16  1.02         0.035             0.0543             0.0456   \n",
       "16  Fund_ 17  1.08         0.035             0.0543             0.0525   \n",
       "17  Fund_ 18  0.98         0.035             0.0543             0.0422   \n",
       "18  Fund_ 19  0.95         0.035             0.0543             0.0463   \n",
       "19  Fund_ 20  0.71         0.035             0.0543             0.0373   \n",
       "20  Fund_ 21  0.99         0.035             0.0543             0.0351   \n",
       "21  Fund_ 22  1.50         0.035             0.0543             0.0147   \n",
       "22  Fund_ 23  0.99         0.035             0.0543             0.0426   \n",
       "23  Fund_ 24  0.97         0.035             0.0543             0.0447   \n",
       "24  Fund_ 25  1.19         0.035             0.0543             0.0530   \n",
       "25  Fund_ 26  1.08         0.035             0.0543             0.0242   \n",
       "26  Fund_ 27  1.11         0.035             0.0543             0.0435   \n",
       "27  Fund_ 28  0.92         0.035             0.0543             0.0505   \n",
       "28  Fund_ 29  1.52         0.035             0.0543             0.0694   \n",
       "29  Fund_ 30  0.79         0.035             0.0543             0.0254   \n",
       "30  Fund_ 31  1.20         0.035             0.0543             0.0346   \n",
       "31  Fund_ 32  1.13         0.035             0.0543             0.0531   \n",
       "32  Fund_ 33  1.15         0.035             0.0543             0.0505   \n",
       "33  Fund_ 34  1.03         0.035             0.0543             0.0469   \n",
       "34  Fund_ 35  0.96         0.035             0.0543             0.0448   \n",
       "35  Fund_ 36  1.22         0.035             0.0543             0.0558   \n",
       "36  Fund_ 37  0.99         0.035             0.0543             0.0427   \n",
       "37  Fund_ 38  1.20         0.035             0.0543             0.0356   \n",
       "38  Fund_ 39  0.98         0.035             0.0543             0.0455   \n",
       "39  Fund_ 40  1.01         0.035             0.0543             0.0487   \n",
       "40  Fund_ 41  1.22         0.035             0.0543             0.0523   \n",
       "41  Fund_ 42  1.05         0.035             0.0543             0.0492   \n",
       "42  Fund_ 43  1.04         0.035             0.0543             0.0437   \n",
       "43  Fund_ 44  1.50         0.035             0.0543             0.0462   \n",
       "44  Fund_ 45  0.95         0.035             0.0543             0.0477   \n",
       "45  Fund_ 46  1.50         0.035             0.0543             0.0376   \n",
       "46  Fund_ 47  1.50         0.035             0.0543             0.0458   \n",
       "47  Fund_ 48  1.14         0.035             0.0543             0.0555   \n",
       "48  Fund_ 49  0.97         0.035             0.0543             0.0461   \n",
       "49  Fund_ 50  0.83         0.035             0.0543             0.0459   \n",
       "\n",
       "     returns  \n",
       "0  -0.057763  \n",
       "1   0.023262  \n",
       "2  -0.027454  \n",
       "3   0.040554  \n",
       "4  -0.034595  \n",
       "5   0.029320  \n",
       "6   0.023787  \n",
       "7   0.019570  \n",
       "8   0.010891  \n",
       "9   0.027380  \n",
       "10  0.001506  \n",
       "11  0.039334  \n",
       "12 -0.018080  \n",
       "13  0.042553  \n",
       "14  0.053575  \n",
       "15  0.011799  \n",
       "16 -0.033941  \n",
       "17  0.026713  \n",
       "18  0.017146  \n",
       "19  0.018682  \n",
       "20  0.019504  \n",
       "21  0.020009  \n",
       "22  0.055120  \n",
       "23  0.040455  \n",
       "24  0.057316  \n",
       "25  0.025105  \n",
       "26  0.027650  \n",
       "27  0.014087  \n",
       "28  0.027902  \n",
       "29  0.098552  \n",
       "30  0.026166  \n",
       "31  0.044948  \n",
       "32  0.014087  \n",
       "33  0.041309  \n",
       "34  0.051406  \n",
       "35  0.023485  \n",
       "36  0.025625  \n",
       "37  0.037007  \n",
       "38  0.017190  \n",
       "39  0.047663  \n",
       "40  0.037782  \n",
       "41  0.044438  \n",
       "42  0.051556  \n",
       "43  0.039726  \n",
       "44  0.002745  \n",
       "45  0.044673  \n",
       "46  0.050250  \n",
       "47  0.028529  \n",
       "48  0.044205  \n",
       "49  0.040828  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    return df\n",
    "\n",
    "\n",
    "clean_column_names(data_file_page_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pandas Inner Join\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the columns of the data frame 1\n",
    "data_file_page_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "data_file_page_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner join is a type of join that returns only the rows that have matching values in both tables. This means that if the two tables have a row with the same value in the column that we are joining on, that row will be included in the result set. \n",
    "\n",
    "The syntax for an pandas inner join is as follows:\n",
    "\n",
    "```python\n",
    "df1.merge(df2, on='column_name', how='inner, suffixes=('_df1', '_df2'))\n",
    "``` \n",
    "\n",
    "where\n",
    "- `df1` and `df2` are the two DataFrames to join\n",
    "- `column_name` is the column name to join the two DataFrames on\n",
    "- `how='inner'` specifies that we want to perform an inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined_inner_join = data_file_page_1.merge(data_file_page_2, on='Fund_id', how='inner').head()\n",
    "data_combined_inner_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape of the combined data\n",
    "data_combined_inner_join.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using suffixes to differentiate the columns\n",
    "\n",
    "We can use the `suffixes` parameter to specify a custom suffix for the columns that have the same name in the two DataFrames. This can be useful when the two DataFrames have columns with the same name that we do not want to join on, but we still want to keep both columns in the result set.  \n",
    "\n",
    "The syntax for an inner join with suffixes is as follows:\n",
    "\n",
    "```python\n",
    "df1.merge(df2, on='column_name', how='inner', suffixes=('_df1', '_df2'))\n",
    "``` \n",
    "\n",
    "where\n",
    "- `df1` and `df2` are the two DataFrames to join\n",
    "- `column_name` is the column name to join the two DataFrames on\n",
    "- `how='inner'` specifies that we want to perform an inner join\n",
    "- `suffixes=('_df1', '_df2')` specifies the suffixes to use for the columns that have the same name in the two DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One to Many Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one to many relationship is a type of relationship where one row in one table can be related to one or more rows in another table. This is achieved by having a foreign key in the many table that references the primary key in the one table. \n",
    "\n",
    "In this case, we can perform an inner join between the two tables on the foreign key column to get the rows that have matching values in both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging multipleDataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sytax for merging multiple DataFrames is as follows:\n",
    "\n",
    "```python\n",
    "df1.merge(df2, on='column_name', how='inner').merge(df3, on='column_name', how='inner')\n",
    "``` \n",
    "\n",
    "where\n",
    "- `df1`, `df2`, and `df3` are the DataFrames to join\n",
    "- `column_name` is the column name to join the DataFrames on\n",
    "- `how='inner'` specifies that we want to perform an inner join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the data page 1 and 2 to get the data for the first group\n",
    "data_combined_multiple_join_1 = data_combined_inner_join.merge(data_file_page_2, on=\"Fund_id\", suffixes=(\"_x\", \"_y\"))\n",
    "data_combined_multiple_join_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the columns of the data\n",
    "data_combined_multiple_join_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data for page 5 of the excel file\n",
    "\n",
    "data_file_page_5 = pd.read_excel(file_path, sheet_name=5)\n",
    "data_file_page_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the columns of the data to determined the columns to merge on\n",
    "data_file_page_5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the data for the first group with the data for page 5\n",
    "data_combined_multiple_join_2 = data_combined_inner_join.merge(data_file_page_2, on=\"Fund_id\", suffixes=(\"_x\", \"_y\"))\\\n",
    ".merge(data_file_page_5, on='RiskFreeRate ')\n",
    "\n",
    "data_combined_multiple_join_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined_multiple_join_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Left Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left join is a type of join that returns all the rows from the left table and the matched rows from the right table. If there is no match, the result is NULL on the right side. The sytax for left join is as follows:\n",
    "\n",
    "```python\n",
    "df1.merge(df2, on='column_name', how='left', suffixes=('_df1', '_df2'))\n",
    "``` \n",
    "\n",
    "where\n",
    "``` \n",
    "\n",
    "where\n",
    "- `df1` and `df2` are the two DataFrames to join\n",
    "- `column_name` is the column name to join the two DataFrames on\n",
    "- `how='left'` specifies that we want to perform a left join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'/Users/teslim/OneDrive/15_data_Ch_4_1.xlsx'\n",
    "data_file_page_4 = pd.read_excel(file, sheet_name=4)\n",
    "\n",
    "data_file_page_4.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Right Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax provided is for merging two pandas DataFrames, `movies` and `tv_genre`, using a right join. The merge operation is performed based on the columns `id` from the `movies` DataFrame and `movie_id` from the `tv_genre` DataFrame. Here is the complete syntax:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming movies and tv_genre are already defined DataFrames\n",
    "# tv_movies = movies.merge(tv_genre, how='right', left_on='id', right_on='movie_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation:\n",
    "- `movies.merge(tv_genre, how='right', left_on='id', right_on='movie_id')`: This line merges the `movies` DataFrame with the `tv_genre` DataFrame.\n",
    "  - `how='right'`: Specifies that a right join should be performed. This means all rows from the `tv_genre` DataFrame will be included, and only matching rows from the `movies` DataFrame will be included.\n",
    "  - `left_on='id'`: Specifies that the merge should be based on the `id` column from the `movies` DataFrame.\n",
    "  - `right_on='movie_id'`: Specifies that the merge should be based on the `movie_id` column from the `tv_genre` DataFrame.\n",
    "\n",
    "### Example:\n",
    "Here is an example with sample data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for movies DataFrame\n",
    "movies = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'title': ['Movie A', 'Movie B', 'Movie C']\n",
    "})\n",
    "\n",
    "print(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for tv_genre DataFrame\n",
    "tv_genre = pd.DataFrame({\n",
    "    'movie_id': [2, 3, 4],\n",
    "    'genre': ['Comedy', 'Drama', 'Action']\n",
    "})\n",
    "\n",
    "print(tv_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the DataFrames\n",
    "tv_movies = movies.merge(tv_genre, how='right', left_on='id', right_on='movie_id')\n",
    "\n",
    "print(tv_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this example, the resulting `tv_movies` DataFrame includes all rows from the `tv_genre` DataFrame and the matching rows from the `movies` DataFrame. Rows from `tv_genre` that do not have a matching `id` in `movies` will have `NaN` values for the columns from `movies`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Merging a Table to itself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1.columns = data_file_page_1.columns.str.lower().str.replace(' ', '_')\n",
    "data_file_page_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = data_file_page_1.merge(data_file_page_1,left_on='returns', \n",
    "                                  right_on='riskfreerate_', suffixes=('x_list', 'y_list'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1[data_file_page_1['fund_id'].isin(data_file_page_1['fund_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filtering joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_4 = pd.read_excel(file_path, sheet_name=4)\n",
    "data_file_page_4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_4.columns = data_file_page_4.columns.str.lower().str.replace(' ', '_')\n",
    "data_file_page_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_6 = pd.read_excel(file_path, sheet_name=6)\n",
    "data_file_page_6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_6.columns = data_file_page_6.columns.str.lower().str.replace(' ', '_')\n",
    "data_file_page_6.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the data for page 4 and 6\n",
    "track_combined = data_file_page_4.merge(data_file_page_6, on='fund_id', how='inner')\n",
    "track_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the element of data 6 that is in the track_combined\n",
    "data_file_page_6['fund_id'].isin(track_combined['fund_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_list = data_file_page_6[data_file_page_6['fund_id'].isin(track_combined['fund_id'])]\n",
    "top_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ConcatenateDataFramestogether vertically\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas `concat` function can be used to concatenate DataFrames together vertically. This means that the rows of the DataFrames are stacked on top of each other to create a new DataFrame. The syntax for concatenating DataFrames vertically is as follows:\n",
    "\n",
    "```python\n",
    "pd.concat([df1, df2], axis=0)\n",
    "``` \n",
    "\n",
    "where\n",
    "- `df1` and `df2` are the DataFrames to concatenate\n",
    "- `axis=0` specifies that the concatenation should be done along the row axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = r'/Users/teslim/OneDrive/18-mutual_raw_data.xls'\n",
    "\n",
    "# reading the data from the excel file\n",
    "df_page_1 = pd.read_excel(file_path_1, sheet_name=4)\n",
    "df_page_2 = pd.read_excel(file_path_1, sheet_name=5)\n",
    "df_page_3 = pd.read_excel(file_path_1, sheet_name=6)\n",
    "df_page_4 = pd.read_excel(file_path_1, sheet_name=7)\n",
    "df_page_5 = pd.read_excel(file_path_1, sheet_name=8)\n",
    "df_page_6 = pd.read_excel(file_path_1, sheet_name=9)\n",
    "df_page_7 = pd.read_excel(file_path_1, sheet_name=10)\n",
    "df_page_8 = pd.read_excel(file_path_1, sheet_name=11)\n",
    "df_page_9 = pd.read_excel(file_path_1, sheet_name=12)\n",
    "df_page_10 = pd.read_excel(file_path_1, sheet_name=13)\n",
    "df_page_11 = pd.read_excel(file_path_1, sheet_name=14)\n",
    "df_page_12 = pd.read_excel(file_path_1, sheet_name=15)\n",
    "df_page_13 = pd.read_excel(file_path_1, sheet_name=16)\n",
    "df_page_14 = pd.read_excel(file_path_1, sheet_name=17)\n",
    "df_page_15 = pd.read_excel(file_path_1, sheet_name=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_all = pd.concat([df_page_1, df_page_2, df_page_3, df_page_4, \n",
    "                    df_page_5, df_page_6, df_page_7, df_page_8, \n",
    "                    df_page_9, df_page_10, df_page_11, df_page_12, \n",
    "                    df_page_13, df_page_14, df_page_15], axis=0)\n",
    "\n",
    "pd_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of concatenating DataFrames vertically one by one, we can also concatenate multiple DataFrames together at once by passing a list of DataFrame as function as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path_1 = r'/Users/teslim/OneDrive/18-mutual_raw_data.xls'\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop through the sheet numbers and read each sheet\n",
    "for sheet_num in range(4, 21):\n",
    "    df = pd.read_excel(file_path_1, sheet_name=sheet_num)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Optionally, concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(df_list, ignore_index=True, sort=False)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the combined data\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The purpose of the `ignore_index` parameter\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ignore_index=True` parameter in the `pd.concat()` function is used to reset the index of the resulting concatenated DataFrame. When you concatenate multiple DataFrames, each DataFrame retains its original index by default. This can lead to duplicate or non-sequential indices in the combined DataFrame. Setting `ignore_index=True` ensures that the index of the resulting DataFrame is reset to a default integer index, which starts from 0 and increments sequentially.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider two DataFrames:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': ['a', 'b', 'c']\n",
    "}, index=[0, 1, 2])\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'A': [4, 5, 6],\n",
    "    'B': ['d', 'e', 'f']\n",
    "}, index=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you concatenate these DataFrames without `ignore_index=True`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df1, df2])\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notice that the index is not unique and retains the original indices from `df1` and `df2`.\n",
    "\n",
    "If you concatenate these DataFrames with `ignore_index=True`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this case, the index is reset to a default integer index, ensuring that it is unique and sequential.\n",
    "\n",
    "Summary:\n",
    "Using `ignore_index=True` is particularly useful when you want to combine multiple DataFrames and ensure that the resulting DataFrame has a clean, sequential index. This is often desirable in data analysis tasks where a consistent and unique index is important for further processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting labels to original tables\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we ignore the index, we can set the labels of the original tables by using the `keys` parameter. The syntax for this is as follows:\n",
    "\n",
    "```python\n",
    "pd.concat([df1, df2], axis=0, ignore_index=False, keys=['df1', 'df2'])\n",
    "``` \n",
    "\n",
    "where\n",
    "- `df1` and `df2` are the DataFrames to concatenate\n",
    "- `axis=0` specifies that the concatenation should be done along the row axis\n",
    "- `keys=['df1', 'df2']` specifies the labels to use for the original tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_1 = pd.concat([df1, df2], ignore_index=False, keys=['G1', 'G2'])\n",
    "combined_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the data data_page 5 and 6 when the ignore_index is set to True\n",
    "all = pd.concat([df_page_5, df_page_6], axis=0, ignore_index=True)\n",
    "all.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the data data_page 5 and 6 when the ignore_index is set to false\n",
    "all2 = pd.concat([df_page_5, df_page_6], axis=0, ignore_index=False, keys=['label 1', 'label 2'])\n",
    "all2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate tables with different column names\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate tables with different column names by using the `join` parameter. The syntax for this is as follows:\n",
    "\n",
    "```python\n",
    "pd.concat([df1, df2], axis=0, join='inner')\n",
    "``` \n",
    "\n",
    "where\n",
    "- `df1` and `df2` are the DataFrames to concatenate\n",
    "- `axis=0` specifies that the concatenation should be done along the row axis\n",
    "- `join='inner'` specifies that only the columns that are common to both DataFrames should be included in the result\n",
    "\n",
    "Note that is will only result the columns that are common to both DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check the data  and know which one to c\n",
    "data_file_page_4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check the data  and know which one to c\n",
    "data_file_page_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combing the data for page 4 and 6\n",
    "data_join = pd.concat([data_file_page_4, data_file_page_2], axis=0, ignore_index=True, join='inner')\n",
    "data_join.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of concatenating tables with different column names is shown below is using the `sort` parameter. The syntax for this is as follows:    \n",
    "\n",
    "```python\n",
    "pd.concat([df1, df2], axis=0, sort=False)\n",
    "``` \n",
    "\n",
    "where\n",
    "- `df1` and `df2` are the DataFrames to concatenate\n",
    "- `axis=0` specifies that the concatenation should be done along the row axis   \n",
    "- `sort=False` specifies that the columns should not be sorted in the result\n",
    "\n",
    "This example shows how to concatenate two DataFrames with different column names without sorting the columns in the result. By default, the columns are sorted in the result, but setting `sort=False` prevents this sorting. Again, this shows every columns in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_join_1 = pd.concat([data_file_page_4, data_file_page_2], axis=0, ignore_index=True, sort=True)\n",
    "data_join_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between the `merge` and `concat` functions in pandas?\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `pandas`, both `merge` and `concat` are used to combine DataFrames, but they serve different purposes and are used in different scenarios. Here's a detailed explanation of when to use each:\n",
    "\n",
    "### `merge`:\n",
    "The `merge` function is used to combine two DataFrames based on one or more keys (columns). It is similar to SQL joins and is used when you need to combine DataFrames based on common columns or indices.\n",
    "\n",
    "#### When to Use `merge`:\n",
    "1. **Relational Data**: When you have relational data and need to combine DataFrames based on common columns or indices.\n",
    "2. **SQL-like Joins**: When you need to perform SQL-like join operations (inner join, outer join, left join, right join).\n",
    "3. **Complex Join Logic**: When you need to specify complex join logic, such as joining on multiple columns or using different column names in each DataFrame.\n",
    "\n",
    "#### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'key': ['A', 'B', 'C'],\n",
    "    'value1': [1, 2, 3]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'key': ['A', 'B', 'D'],\n",
    "    'value2': [4, 5, 6]\n",
    "})\n",
    "\n",
    "# Merge DataFrames on the 'key' column\n",
    "merged_df = pd.merge(df1, df2, on='key', how='inner')\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### `concat`:\n",
    "The `concat` function is used to concatenate DataFrames along a particular axis (rows or columns). It is used when you need to stack DataFrames either vertically (row-wise) or horizontally (column-wise).\n",
    "\n",
    "#### When to Use `concat`:\n",
    "1. **Stacking DataFrames**: When you need to stack DataFrames vertically (one below the other) or horizontally (side by side).\n",
    "2. **Appending DataFrames**: When you need to append one DataFrame to another.\n",
    "3. **Combining Along an Axis**: When you need to combine DataFrames along a specific axis without considering keys or indices.\n",
    "\n",
    "#### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': ['a', 'b', 'c']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'A': [4, 5, 6],\n",
    "    'B': ['d', 'e', 'f']\n",
    "})\n",
    "\n",
    "# Concatenate DataFrames vertically (row-wise)\n",
    "concat_df = pd.concat([df1, df2], axis=0)\n",
    "print(concat_df)\n",
    "print()\n",
    "\n",
    "# Concatenate DataFrames horizontally (column-wise)\n",
    "concat_df_horizontal = pd.concat([df1, df2], axis=1)\n",
    "print(concat_df_horizontal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary:\n",
    "- **Use `merge`**:\n",
    "  - When you need to combine DataFrames based on common columns or indices.\n",
    "  - When performing SQL-like join operations (inner, outer, left, right joins).\n",
    "  - When you need to specify complex join logic.\n",
    "\n",
    "- **Use `concat`**:\n",
    "  - When you need to stack DataFrames vertically or horizontally.\n",
    "  - When appending one DataFrame to another.\n",
    "  - When combining DataFrames along a specific axis without considering keys or indices.\n",
    "\n",
    "By understanding the differences and use cases for `merge` and `concat`, you can choose the appropriate method for combining DataFrames based on your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verifying Integrity When Joining Data with Pandas\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with data, especially in data science and analytics, joining datasets is a common task. However, ensuring the integrity of your data during these joins is crucial to avoid introducing errors or inconsistencies. This section will provide a comprehensive overview of how to verify data integrity when joining data using Pandas.\n",
    "\n",
    "####  Key Concepts in Data Integrity Verification\n",
    "\n",
    "1. Understanding Joins:\n",
    "\n",
    "* Inner Join: Returns rows that have matching values in both DataFrames.\n",
    "* Left Join: Returns all rows from the left DataFrame and matched rows from the right DataFrame; unmatched rows have NaN.\n",
    "* Right Join: Returns all rows from the right DataFrame and matched rows from the left DataFrame; unmatched rows have NaN.\n",
    "* Outer Join: Returns all rows when there is a match in either DataFrame; unmatched areas are filled with NaN.\n",
    "\n",
    "2. Data Integrity Issues in Joins:\n",
    "\n",
    "* Duplicated Keys: Occurs when there are duplicate values in the key column(s), leading to unintended row duplication in the merged DataFrame.\n",
    "* Missing Keys: Can result in missing data when performing joins; for instance, missing rows in an inner join or missing values in left or right joins.\n",
    "* Incorrect Matching: When joins do not behave as expected due to data types or incorrect columns being used.\n",
    "\n",
    "3. Verifying Join Integrity in Pandas: \n",
    "Pandas provides built-in mechanisms to ensure the integrity of joins, preventing errors from unnoticed duplicates or missing values.\n",
    "\n",
    "* Using validate Parameter in merge: The validate parameter allows you to check the assumptions of your merge operation:\n",
    "- 'one_to_one': Ensures that both DataFrames have unique values in the join columns.\n",
    "\n",
    "- 'one_to_many': Ensures that the left DataFrame has unique values in the join column, but the right DataFrame can have duplicates.\n",
    "\n",
    "- 'many_to_one': Ensures that the right DataFrame has unique values in the join column, but the left DataFrame can have duplicates.\n",
    "\n",
    "- 'many_to_many': Allows duplicates in both DataFrames (default behavior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax for using the validate parameter is as follows:\n",
    "\n",
    "```python\n",
    "df1.merge(df2, on='column_name', how='inner', validate='one_to_one')\n",
    "```\n",
    "where df1 and df2 are the DataFrames to join, column_name is the column name to join the DataFrames on, how='inner' specifies that we want to perform an inner join, and validate='one_to_one' ensures that the join is one-to-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'key': ['A', 'B', 'B'], 'value2': [4, 5, 6]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with validation\n",
    "merged_df = pd.merge(df1, df2, on='key', how='inner', validate='one_to_one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code would raise an error because df2 has duplicated values for 'B', violating the one_to_one validation rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Common Integrity Issues:\n",
    "\n",
    "1. Duplicated Keys: Use the `drop_duplicates()` method before merging if you want to ensure unique keys in your DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop_duplicates(subset=['key'], inplace=True)\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Missing Keys: After merging, you can use the `isnull()` function to check for missing values, indicating unmatched rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = merged_df.isnull().any(axis=1)\n",
    "print(merged_df[missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Type Consistency: Ensure that the data types of your joining keys are consistent across DataFrames to avoid unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types of the data\n",
    "print(df1.dtypes)\n",
    "print(df2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Data Types if Necessary: Convert 'key' to string \n",
    "df1['key'] = df1['key'].astype(str)\n",
    "df2['key'] = df2['key'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Assessing Join Results: After performing a join, its essential to assess the merged DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Row Count: {len(df1)}, Merged Row Count: {len(merged_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying concatenations\n",
    "\n",
    "The `verify_integrity` parameter in the `concat()` function allows you to check for duplicate indices in the concatenated DataFrame. By setting `verify_integrity=True`, you can ensure that the resulting DataFrame has unique indices, preventing potential issues. The default value for `verify_integrity` is `False`, meaning that the check is not performed by default.\n",
    "\n",
    "The syntax for using the `verify_integrity` parameter is as follows:\n",
    "\n",
    "```python\n",
    "pd.concat([df1, df2], verify_integrity=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_page_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_page_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the data for page 1 and 2 with the verification of the data \n",
    "verify = pd.concat([df_page_1, df_page_2], axis=1, ignore_index=True, verify_integrity=True)\n",
    "verify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the data for page 1 and 2 with the verification of the data \n",
    "verify = pd.concat([df_page_1, df_page_2], axis=0, ignore_index=True, verify_integrity=False)\n",
    "verify.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Using merge_ordered()\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `merge_ordered()` function in pandas is used to merge two DataFrames based on the order of the keys. This function is particularly useful when you need to merge time series or ordered data where the order of the keys is important.\n",
    "\n",
    "The syntax for using `merge_ordered()` is as follows:\n",
    "\n",
    "```python\n",
    "pd.merge_ordered(df1, df2, on='column_name', how='outer', fill_method='ffill')\n",
    "```\n",
    "\n",
    "where\n",
    "- `df1` and `df2` are the DataFrames to merge\n",
    "- `column_name` is the column to merge on\n",
    "- `how='outer'` specifies that an outer join should be performed\n",
    "- `fill_method='ffill'` specifies the method to use for filling missing values\n",
    "\n",
    "The `merge_ordered()` function is similar to the `merge()` function in pandas, but it is specifically designed for merging ordered data. It allows you to merge two DataFrames based on the order of the keys, which can be useful when working with time series data or other ordered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = r'/Users/teslim/OneDrive/generaldata/18-mutual_raw_data.xls'\n",
    "\n",
    "df_10_page_8 = pd.read_excel(file_path_1, sheet_name=9)\n",
    "df_10_page_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_page_8.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_page_9 = pd.read_excel(file_path_1, sheet_name=10)\n",
    "df_10_page_9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the stock data for page 8 and 9\n",
    "\n",
    "stock = pd.merge_ordered(df_10_page_8, df_10_page_9, on='Date', suffixes=('_8', '_9'), fill_method='ffill')\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_merge = df_10_page_8.merge(df_10_page_9, on='Date')\n",
    "stock_merge.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10_page_8[['Open', 'Adj Close']].plot(x = 'Open', y = 'Adj Close', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Using merge_asof( )\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `merge_asof()` function in `pandas` is used to perform an asof merge, which is similar to a left join but for ordered data. This function is particularly useful for time series data where you want to merge on the nearest key rather than an exact match.\n",
    "\n",
    "### Key Points:\n",
    "- **Ordered Data**: The data must be ordered by the key before performing the merge.\n",
    "- **Nearest Key**: Merges on the nearest key rather than an exact match.\n",
    "- **Tolerance**: You can specify a tolerance to limit the distance between keys.\n",
    "\n",
    "### Syntax:\n",
    "\n",
    "```python\n",
    "pd.merge_asof(left, right, on=None, left_on=None, right_on=None, \n",
    "              by=None, left_by=None, right_by=None, \n",
    "              suffixes=('_x', '_y'), \n",
    "              tolerance=None, allow_exact_matches=True, direction='backward')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Parameters:\n",
    "- **left**: DataFrame.\n",
    "- **right**: DataFrame.\n",
    "- **on**: Column name to join on. Must be ordered.\n",
    "- **left_on**: Column name from the left DataFrame to join on.\n",
    "- **right_on**: Column name from the right DataFrame to join on.\n",
    "- **by**: Column name to group by before merging.\n",
    "- **left_by**: Column name from the left DataFrame to group by.\n",
    "- **right_by**: Column name from the right DataFrame to group by.\n",
    "- **suffixes**: Suffix to apply to overlapping column names.\n",
    "- **tolerance**: Maximum distance between keys for a match.\n",
    "- **allow_exact_matches**: Whether to allow exact matches.\n",
    "- **direction**: Direction of the merge ('backward', 'forward', or 'nearest').\n",
    "\n",
    "### Example:\n",
    "Here is an example of how to use `merge_asof()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left Sample DataFrames\n",
    "left = pd.DataFrame(\n",
    "    {'time': pd.to_datetime(['2021-01-01 09:00', '2021-01-01 09:01', '2021-01-01 09:02']),\n",
    "    'value': [1, 2, 3]})\n",
    "\n",
    "\n",
    "# Ensure the data is sorted by the key\n",
    "left = left.sort_values('time')\n",
    "\n",
    "print(left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right Sample DataFrames\n",
    "right = pd.DataFrame({\n",
    "    'time': pd.to_datetime(['2021-01-01 09:00', '2021-01-01 09:01', '2021-01-01 09:03']),\n",
    "    'value': [10, 20, 30]\n",
    "})\n",
    "\n",
    "# Ensure the data is sorted by the key\n",
    "right = right.sort_values(by='time')\n",
    "\n",
    "print(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an asof merge\n",
    "merged = pd.merge_asof(left, right, on='time', suffixes=('_left', '_right'))\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation:\n",
    "- **DataFrames**: Two DataFrames `left` and `right` with time series data.\n",
    "- **Sorting**: Ensure both DataFrames are sorted by the key column (`time`).\n",
    "- **Merge**: Use `merge_asof()` to merge on the nearest key (`time`).\n",
    "\n",
    "### Summary:\n",
    "- **`merge_asof()`**: Useful for merging time series data on the nearest key.\n",
    "- **Ordered Data**: Ensure the data is ordered by the key column.\n",
    "- **Parameters**: Customize the merge with parameters like `tolerance` and `direction`.\n",
    "\n",
    "By using `merge_asof()`, you can effectively merge time series data based on the nearest key, making it a powerful tool for handling ordered data in `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's use the [`right`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y235sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A0%7D%7D%5D%2C%22fc59d8c8-fb73-4a24-8135-f82233c44ed6%22%5D \"Go to definition\") DataFrame you provided and create a `left` DataFrame to demonstrate the `merge_asof()` function with both `backward` and `forward` directions.\n",
    "\n",
    "### Sample DataFrames:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Right Sample DataFrame\n",
    "right = pd.DataFrame({\n",
    "    'time': pd.to_datetime(['2021-01-01 09:00', '2021-01-01 09:01', '2021-01-01 09:03']),\n",
    "    'value': [10, 20, 30]\n",
    "})\n",
    "\n",
    "# Left Sample DataFrame\n",
    "left = pd.DataFrame({\n",
    "    'time': pd.to_datetime(['2021-01-01 08:59', '2021-01-01 09:01', '2021-01-01 09:02']),\n",
    "    'value': [1, 2, 3]\n",
    "})\n",
    "\n",
    "# Ensure the data is sorted by the key\n",
    "right = right.sort_values(by='time')\n",
    "left = left.sort_values(by='time')\n",
    "\n",
    "print(\"Right DataFrame:\")\n",
    "print(right)\n",
    "print(\"\\nLeft DataFrame:\")\n",
    "print(left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Backward Merge:\n",
    "In a backward merge, each row in the `left` DataFrame will be matched with the nearest previous row in the [`right`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y235sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A0%7D%7D%5D%2C%22fc59d8c8-fb73-4a24-8135-f82233c44ed6%22%5D \"Go to definition\") DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an asof merge with direction='backward'\n",
    "merged_backward = pd.merge_asof(left, right, on='time', direction='backward', suffixes=('_left', '_right'))\n",
    "print(\"\\nMerged DataFrame (Backward):\")\n",
    "print(merged_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Forward Merge:\n",
    "In a forward merge, each row in the `left` DataFrame will be matched with the nearest subsequent row in the [`right`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y235sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A0%7D%7D%5D%2C%22fc59d8c8-fb73-4a24-8135-f82233c44ed6%22%5D \"Go to definition\") DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an asof merge with direction='forward'\n",
    "merged_forward = pd.merge_asof(left, right, on='time', direction='forward', suffixes=('_left', '_right'))\n",
    "print(\"\\nMerged DataFrame (Forward):\")\n",
    "print(merged_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Complete Example:\n",
    "Here is the complete code snippet:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Right Sample DataFrame\n",
    "right = pd.DataFrame({\n",
    "    'time': pd.to_datetime(['2021-01-01 09:00', '2021-01-01 09:01', '2021-01-01 09:03']),\n",
    "    'value': [10, 20, 30]\n",
    "})\n",
    "\n",
    "# Left Sample DataFrame\n",
    "left = pd.DataFrame({\n",
    "    'time': pd.to_datetime(['2021-01-01 08:59', '2021-01-01 09:01', '2021-01-01 09:02']),\n",
    "    'value': [1, 2, 3]\n",
    "})\n",
    "\n",
    "# Ensure the data is sorted by the key\n",
    "right = right.sort_values(by='time')\n",
    "left = left.sort_values(by='time')\n",
    "\n",
    "print(\"Right DataFrame:\")\n",
    "print(right)\n",
    "print(\"\\nLeft DataFrame:\")\n",
    "print(left)\n",
    "\n",
    "# Perform an asof merge with direction='backward'\n",
    "merged_backward = pd.merge_asof(left, right, on='time', direction='backward', suffixes=('_left', '_right'))\n",
    "print(\"\\nMerged DataFrame (Backward):\")\n",
    "print(merged_backward)\n",
    "\n",
    "# Perform an asof merge with direction='forward'\n",
    "merged_forward = pd.merge_asof(left, right, on='time', direction='forward', suffixes=('_left', '_right'))\n",
    "print(\"\\nMerged DataFrame (Forward):\")\n",
    "print(merged_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation:\n",
    "- **Backward Merge**: Matches each row in `left` with the nearest previous row in [`right`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y235sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A0%7D%7D%5D%2C%22fc59d8c8-fb73-4a24-8135-f82233c44ed6%22%5D \"Go to definition\").\n",
    "- **Forward Merge**: Matches each row in `left` with the nearest subsequent row in [`right`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y235sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A0%7D%7D%5D%2C%22fc59d8c8-fb73-4a24-8135-f82233c44ed6%22%5D \"Go to definition\").\n",
    "\n",
    "By using `merge_asof()` with different `direction` parameters, you can control how the nearest key is selected for merging time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Selecting data with df.query()\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df.query()` function in `pandas` provides a way to query a DataFrame using a boolean expression. This method is useful for filtering rows based on conditions and can make your code more readable and concise compared to using boolean indexing.\n",
    "\n",
    "### Syntax:\n",
    "\n",
    "```python\n",
    "DataFrame.query(expr, inplace=False, **kwargs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Parameters:\n",
    "- **expr**: The query string to evaluate. This string should be a valid Python expression.\n",
    "- **inplace** (optional): If `True`, modifies the DataFrame in place. Default is `False`.\n",
    "- **kwargs**: Additional keyword arguments to pass to the query method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1.columns = data_file_page_1.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "data_file_page_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking a beta value greater than 0.5\n",
    "data_file_page_1.query('beta > 0.5').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking a return value greater than 0.5\n",
    "data_file_page_1.query('returns > 0.05').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# querying on amultple condition usin the and operator\n",
    "data_file_page_1.query('returns > 0.05 and beta > 0.5').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1.query('returns > 0.05 or beta > 0.5').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Reshaping data with df.melt()\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The df.melt() function is used to change the DataFrame format from wide to long. The function takes the following parameters:\n",
    "    \n",
    "```python\n",
    "    pd.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None)\n",
    "```\n",
    "    \n",
    "\n",
    "\n",
    "- **frame**: The DataFrame to be melted.\n",
    "- **id_vars**: Columns to use as identifier variables (columns that will remain unchanged).\n",
    "- **value_vars**: Columns to unpivot (columns that will be melted).\n",
    "- **var_name**: Name to use for the variable column.\n",
    "- **value_name**: Name to use for the value column.\n",
    "- **col_level**: If columns are MultiIndex, use this level to melt.\n",
    "\n",
    "The `melt()` function is useful for reshaping data when you want to convert wide-format data to long-format data. This can be helpful for various data analysis and visualization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading of the dataset \n",
    "uk_election = pd.read_excel(r'/Users/teslim/OneDrive/generaldata/1_uk_2024_election.xlsx', sheet_name=4)\n",
    "uk_election.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the data type of the data\n",
    "uk_election.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meting the data base on the region and constituency\n",
    "uk_wide_data = uk_election.melt(id_vars=['Constituency', 'Region'])\n",
    "uk_wide_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadig the data for from the previous page\n",
    "data_file_page_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melting the data base on the fund_id and beta\n",
    "data_file_page_1.melt(id_vars=['fund_id', 'beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it will be notice that the pandas add variable and value columns to the DataFrame, where the variable column contains the original `column names` that were melted, and the value column contains the corresponding `values`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Melting with value_vars parameter is used to specify the columns that should be melted. This parameter allows you to select specific columns to unpivot, while the remaining columns will remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_page_1.melt(id_vars=['fund_id', 'beta'], value_vars=['returns', 'riskfreerate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Parameters:\n",
    "- **id_vars**: Columns to use as identifier variables.\n",
    "- **value_vars**: Columns to unpivot. If not specified, all columns not set as [`id_vars`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y302sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A22%7D%7D%5D%2C%228436c726-de00-407a-9b3e-04bb3d1e7258%22%5D \"Go to definition\") are used.\n",
    "- **var_name**: Name to use for the variable column. If None, uses `variable`.\n",
    "- **value_name**: Name to use for the value column. If None, uses `value`.\n",
    "\n",
    "### Example DataFrame:\n",
    "Let's assume [`data_file_page_1`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y302sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A0%7D%7D%5D%2C%228436c726-de00-407a-9b3e-04bb3d1e7258%22%5D \"Go to definition\") is as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'fund_id': [1, 2, 3],\n",
    "    'beta': [0.5, 0.6, 0.7],\n",
    "    'returns': [10, 20, 30],\n",
    "    'riskfreerate': [1, 2, 3],\n",
    "    'alpha': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "data_file = pd.DataFrame(data)\n",
    "print(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Using [`melt`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y302sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A17%7D%7D%5D%2C%228436c726-de00-407a-9b3e-04bb3d1e7258%22%5D \"Go to definition\") without `value_vars`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = data_file.melt(id_vars=['fund_id', 'beta'])\n",
    "print(melted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Using [`melt`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y302sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A17%7D%7D%5D%2C%228436c726-de00-407a-9b3e-04bb3d1e7258%22%5D \"Go to definition\") with `value_vars`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = data_file.melt(id_vars=['fund_id', 'beta'], value_vars=['returns', 'riskfreerate'])\n",
    "print(melted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Differences:\n",
    "1. **Without `value_vars`**:\n",
    "   - All columns not specified in [`id_vars`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y302sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A22%7D%7D%5D%2C%228436c726-de00-407a-9b3e-04bb3d1e7258%22%5D \"Go to definition\") are unpivoted.\n",
    "   - In the example, columns `returns`, `riskfreerate`, and `alpha` are unpivoted.\n",
    "\n",
    "2. **With `value_vars`**:\n",
    "   - Only the columns specified in `value_vars` are unpivoted.\n",
    "   - In the example, only columns `returns` and `riskfreerate` are unpivoted, and `alpha` is excluded.\n",
    "\n",
    "### Summary:\n",
    "- **Without `value_vars`**: All columns not in [`id_vars`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22vscode-notebook-cell%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2FUsers%2Fteslim%2FLibrary%2FCloudStorage%2FOneDrive-TeslimUthmanAdeyanju%2FTeslim_data_science_study%2Fnote_3_python_language%2FTeslim_python_study_note%2FTeslim_python_datacamp%2Fpython_data_camp_code_workspace%2FSession-4-Joining_data_with_pandas.ipynb%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22Y302sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A1%2C%22character%22%3A22%7D%7D%5D%2C%228436c726-de00-407a-9b3e-04bb3d1e7258%22%5D \"Go to definition\") are melted.\n",
    "- **With `value_vars`**: Only the specified columns in `value_vars` are melted.\n",
    "\n",
    "Using `value_vars` allows you to control which columns are unpivoted, providing more flexibility in reshaping your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the variable and value column names by using the `var_name` and `value_name` parameters in the `melt()` function. The syntax for this is as follows:\n",
    "\n",
    "```python\n",
    "pd.melt(frame, id_vars=['id'], value_vars=['A', 'B'], var_name='variable', value_name='value')\n",
    "``` \n",
    "\n",
    "where\n",
    "- `frame` is the DataFrame to melt\n",
    "- `id_vars=['id']` specifies the columns to use as identifier variables\n",
    "- `value_vars=['A', 'B']` specifies the columns to unpivot\n",
    "- `var_name='variable'` specifies the name to use for the variable column\n",
    "- `value_name='value'` specifies the name to use for the value column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt the data for the uk election data\n",
    "uk_election.melt(id_vars=['Constituency', 'Region'], value_vars=['GE2019 Implied results: Electorate'], \n",
    "                 var_name= 'Green Policy', value_name='Green Policy Value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
